{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Introduction_of_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Deep-Learning-with-Keras/blob/master/Introduction_of_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# **1-Introduction Keras**[1]\n",
        "## 1.1 - **What is keras**\n",
        "\n",
        "Keras is a deep-learning framework that provides a Convenient way to define and train almost any kind of Deep leanring model. It is written in Python and can be run on top of Tensorflow , CNTK, or Theano. you are free to use it in commerical projects since it is distributed under the MIT licens [1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-OS-JtDaAzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Deep-Learning-with-Keras.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5li5TZBz4NQ",
        "colab_type": "text"
      },
      "source": [
        "## 1.2- **What makes Keras so popular** \n",
        "once of the most important characteristic of kearas is it user-friendly API. You could develop a start of art DL Model in no time . Therefore it is easy and fast prototyping.In addition, it support many modern DL layers such as Convolutional and recurrent layers.Keras layers can be added sequentially or many different combinations in very easy wasy. Regarding hardware, you can run keras on CPU and GPU and switch between them in very easy way[1] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "## 1.3-  **Installing Keras** \n",
        "The installation process is very easy. First, we need to install the backedn where  all the calculation take place (we will choose tensorFlow). Then we install keras [1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLq1Pb9z9bk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ktand8m9iKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is simple as this. Let us test the implementation \n",
        "python -c 'import keras; print(keras._version_)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN",
        "colab_type": "text"
      },
      "source": [
        "## 1.4- **Kera Workflow**#\n",
        "\n",
        "In order to build DL project in keras you normally would follow the following workk flow [1]:\n",
        "- 1) Define your training data\n",
        "- 2) Define your network \n",
        "- 3) Configure the learning process by choosing \n",
        "      - 1) optimizer \n",
        "      - 2) Metrics \n",
        "- 4) iterate over the training data and start fitting your model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a21M_iV6uN_M",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1eVwjExOmSWHL6nFcvsWhCxy6aokQNXQq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aCyhDgoTRK",
        "colab_type": "text"
      },
      "source": [
        "## 1.5- **Keras Models?** \n",
        "The core data structure of keras is the model class. it is found under keras.model that gives you two ways to define models:\n",
        " - Sequential class\n",
        " - Model class\n",
        "\n",
        "The sequential class builds the network layers by layers  in a sequential order . The model class allows for more Complext network structures[1]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq4DpmTeRNNy",
        "colab_type": "text"
      },
      "source": [
        "## 1.6- **Model lifecycle**\n",
        "A Keras model follows the following lifecycle [1]\n",
        "\n",
        " -1. Modle creation\n",
        "\n",
        "     - Define a model using the sequential or model class\n",
        "     - Add the layers \n",
        "\n",
        " -2. Configure the model by specifying the loss, optimizer and metric. This is done by calling the compile method\n",
        "\n",
        " -3. Train the model by calling the fit method\n",
        " \n",
        " -4. By then you will have a trained model that you could use for evaluation or prediction on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOsOIr2HuwEu",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1QghXXdHiADg72Szq7843cDaXKkp7fRUW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVaahYFRsEF",
        "colab_type": "text"
      },
      "source": [
        "## 1.7- **Core layers**\n",
        "keras supports many layers for building our neural network. They accessible from **keras.layers** and the following shows the most basic classes we are going to use [1]:\n",
        "\n",
        " - Dense: is the standard layer of fully connected neuron to the pervious layer. It implemented the operation output =activation (X*W+Bias)\n",
        " - Activation: Applies an activation function to an output \n",
        " - Dropout: applies dropout to the input. Basically , it work randomly deactivation a set of neurons in a given layer according to a predefined probability rate. Drouput is used to prevent overfitting \n",
        " - Conv2D: applies a 2D Convolution to train a set of kernels mainly on image database\n",
        " - Flatten: Flattens the input into ID matrix. Mainly used after feature extraction in Convolution Neural network     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C_dTcsnSGFD",
        "colab_type": "text"
      },
      "source": [
        "## 1.8-**Loss and Optimizers**\n",
        "After defining a model, we need to select a loss function and an optimizer. The optimizer's jobs is to find the best model parameters that minimizes the losss function[1].\n",
        "\n",
        "Avalilable optimizers: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "\n",
        "Available loss Functions: mean absolut error, mean absolute percentage error, mean squared. Logarithmic error , Squared hing, Categorical hinge, logcosh,categorical crossentropy, sparse Categorical crossentropy, binary corssentropy , kullback divergence, poisson, costine proximity \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWjlZqTBTeD7",
        "colab_type": "text"
      },
      "source": [
        "## 1.9. **Keras Utils** \n",
        "Keras provides additional utility functions that facilitates building and viewing models. We will mainly use the them to preprocess data and viewing models [1] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOLixKnwiYkE",
        "colab_type": "text"
      },
      "source": [
        "#**2-Data Pre-processing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNBeXQIhvbKB",
        "colab_type": "text"
      },
      "source": [
        "## **2.1-Dealing with Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwuJEzGMvqDe",
        "colab_type": "text"
      },
      "source": [
        "**Motivation**\n",
        "\n",
        "Training deep learning models requires data… A lot of data! Unfortunately, in most cases data comes messy, and our models are very sensitive towards this. Therefore, we need to be careful while preparing our data to achieve the best results.Get your laptops ready, we have a lot of preprocessing to do [3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcl915YwRoh",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 Working with Numerical Data**\n",
        "\n",
        "Numerical values are going to be the most frequent data types you are going to deal with. Even though they are already in a suitable format for calculations, we still need to do some work.\n",
        "\n",
        "The main problem with numerical data is the different scales each feature holds. Consider a housing prices dataset with information about: house size, number of bedrooms, construction year, and price. Suppose our goal is to predict the price given the house size, number of bedrooms and construction year. Each of those features is presented on a different scale. A house size may range let us say between 100 and 500 meters squared, construction year is a 4 digits number that goes back around 200 years ago, and finally the number of bedrooms is a number between 1-4 [3].\n",
        "\n",
        "The issue here is that a model may give more attention to one feature based on its value. This way the house size may get more attention since its values are bigger, and other important features such as number of bedrooms may be neglected since their values are small [3].\n",
        "\n",
        "Well, do not panic! We have two simple solutions for this problem, they are called Normalization and Standardization [3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGMKB3xlxPkb",
        "colab_type": "text"
      },
      "source": [
        "**Normalization**\n",
        "\n",
        "Normalization simply scales the values in the range [0-1]. To apply it on a dataset you just have to subtract the minimum value from each feature and divide it with the range (max – min).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8TZNiORxvHy",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1DsPxs-4x_BTRfXzjE8Pu_O9dsCYfshff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63E6i9mcx45k",
        "colab_type": "text"
      },
      "source": [
        "**Standardization**\n",
        "\n",
        "Standardization on the other hand transforms data to have a zero mean and one unit standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAhobx7lyJq1",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1jaqkqDWPNebx1rrp191w8D4NB5IkbwqK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cha3yOU2y06q",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Implementing the above techniques in Keras is easier than you think. We will show you an example using the Boston Housing dataset that can be easily loaded with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROO7qWcpy9a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "# data is returned as a tuple for the training and the testing datasets\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvfq35MzLkU",
        "colab_type": "text"
      },
      "source": [
        "Let us look at the first example in the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dMYQKpzOqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnphcnsdzYWc",
        "colab_type": "text"
      },
      "source": [
        "See the different scales? To solve this we will use the popular Scikit-Learn library.\n",
        "\n",
        "Use the MinMaxScaler for data normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2Gv7bizfPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X_train)\n",
        "print(X_normalized[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDG6Zh-zvYj",
        "colab_type": "text"
      },
      "source": [
        "OR, use the StandardScaler to standardize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8RC5Wk1zxE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "print(X_scaled[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WngdvLztz8Qt",
        "colab_type": "text"
      },
      "source": [
        "## **2.4 Working with Categorical Data**[3]\n",
        "\n",
        "Categorical data need special treatment because they can not be fed to a neural network in their own format (Since neural networks only accept numerical data types).\n",
        "\n",
        "We will introduce two main techniques for handling categorical data: Indexing and OneHotEncoding.\n",
        "\n",
        "**Indexing**\n",
        "\n",
        "Indexing is simply replacing a category name with an index or a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UtrHLda6vd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data = np.array(['small', 'medium', 'small', 'large', 'xlarge', 'large'])\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "data_encoded = encoder.fit_transform(data)\n",
        "print(data_encoded)\n",
        "# Output\n",
        "#array([2, 1, 2, 0, 3, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMlKj6wb6_6G",
        "colab_type": "text"
      },
      "source": [
        "Values ‘small’, ‘medium’, ‘large’, and ‘xlarge’ where replaced by numbers from 0 to 3. To strictly specify the numbers used you may refer to OrdinalEncoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCl3Up6b7INQ",
        "colab_type": "text"
      },
      "source": [
        "**OneHotEncoding**[3]\n",
        "\n",
        "OneHotEncoding is replacing each element by a list of boolean values with 1 in the present category index and 0 in the others.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIKagqd67Ut2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array(['red', 'blue', 'orange', 'white', 'red', 'orange', 'white', 'red'])\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "data_encoded = encoder.fit_transform(data)\n",
        "\n",
        "print(data_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8n-JdPH7goO",
        "colab_type": "text"
      },
      "source": [
        "Instead of replacing each color with a number, it was replaced with a list. The list represents the following: is_blue, is_orange, is_red, and is_white. The value 1 is added to the relative index of each color presence and 0 is added otherwise. Example: The color red is represented by: [0 0 1 0]. This technique is used to break the ordinal relation between number if present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHpJ7G167yku",
        "colab_type": "text"
      },
      "source": [
        "## **2.5 Working with Images [3]**\n",
        "\n",
        "We will discuss only two simple image manipulations in this post. More advanced techniques will be introduced later.\n",
        "\n",
        "In the first phase of this tutorial, we will deal with images as simple array of flat consecutive pixels. But images are normally 2 dimensional with 1 or 3 color channels. Therefore we need to reshape each image as an array before we use it. This is done via the reshape function in Numpy.\n",
        "\n",
        "Suppose we have a list of 500 images each with 28 * 28 pixels and 3 color channels RGB. This list needs to be reshaped into (500, 2352) in order to be fed to the network. 2353 here is the size of each image after resize (28 * 28 * 3).\n",
        "\n",
        "Reshaping this list is very easy using Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHlddBWp9T9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_reshaped = data.reshape(500, 28*28*3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSLo_Rtk9hda",
        "colab_type": "text"
      },
      "source": [
        "Simple! Now since our pixels are numeric values, we need to scale them as well. One simple scaling technique for images is to divide each pixel with 255 (the maximum value for each pixel)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Sy-j1s9j3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = images / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj4qo0uT-CaN",
        "colab_type": "text"
      },
      "source": [
        "#**3-Regression with Keras[2]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzXUem-Ot5c5",
        "colab_type": "text"
      },
      "source": [
        "After two introductory tutorials, its time to build our first neural network! The network we are building solves a simple regression problem. Regression is a process where a model learns to predict a continuous value output for a given input data, e.g. predict price, length, width, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9nGFk0AuD2v",
        "colab_type": "text"
      },
      "source": [
        "## **3.1 Problem Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4zWbWluuOe5",
        "colab_type": "text"
      },
      "source": [
        "Our objective is to build prediction model that predicts housing prices from a set of house features. We will use the Boston Housing dataset, which is collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive, and has been used extensively throughout the literature to benchmark algorithms.\n",
        "\n",
        "The dataset is small in size with only 506 cases. It contains 14 features described as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvyXeFFnucBg",
        "colab_type": "text"
      },
      "source": [
        "- CRIM: per capita crime rate by town\n",
        "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "- INDUS: proportion of non-retail business acres per town.\n",
        "- CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "- NOX: nitric oxides concentration (parts per 10 million)\n",
        "- RM: average number of rooms per dwelling\n",
        "- AGE: proportion of owner-occupied units built prior to 1940\n",
        "- DIS: weighted distances to five Boston employment centres\n",
        "- RAD: index of accessibility to radial highways\n",
        "- TAX: full-value property-tax rate per $10,000\n",
        "- pupil-teacher ratio by town\n",
        "- B: 1000(Bk — 0.63)² where Bk is the proportion of blacks by town\n",
        "- LSTAT: % lower status of the population\n",
        "- MEDV: Median value of owner-occupied homes in $1000’s\n",
        "\n",
        "The goal behind our regression problem is to use the 13 features to predict the value of MEDV (which represents the housing price)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdpi_K4rvCfp",
        "colab_type": "text"
      },
      "source": [
        "## **3.2 Loading the Data**\n",
        "\n",
        "Fortunately, Keras has a set of datasets already available. You can access them from keras.dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5YHKhOgvXGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwbel5atvzmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train[0], y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZL8kvtv7xv",
        "colab_type": "text"
      },
      "source": [
        "The data is returned as two tuples representing the training and testing splits. The X_train and X_test contain the feature columns, while the y_train and y_test contain the label/output column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT086MDYv__8",
        "colab_type": "text"
      },
      "source": [
        "## **3.3 Preprocessing**\n",
        "\n",
        "As discussed in the previous article, we need to preprocess our data before feeding it to the network. Obviously, our data needs to be rescaled. Time for our buddy (StandarScaler) from the scikit-learn package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv_UPBj5wW6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# first we fit the scaler on the training dataset\n",
        "scaler.fit(X_train)\n",
        "# then we call the transform method to scale both the training and testing data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# a sample output\n",
        "print(X_train_scaled[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qErxmpKTwkDn",
        "colab_type": "text"
      },
      "source": [
        "Much better! Note that we only rescale the features and not the label column. This dataset is simple and no further preprocessing is needed. Time for the most exciting part…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWsjth6wwip",
        "colab_type": "text"
      },
      "source": [
        "## **3.4 Building the Model**\n",
        "\n",
        "We will build the model layer by layer in a sequential manner. To do so we have to import 1) the model class 2) and the layer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VUuAxNBw-02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models, layers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4iXryMaxFrO",
        "colab_type": "text"
      },
      "source": [
        "Then, we create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69k6lIByxJsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TrlFvL8xNtl",
        "colab_type": "text"
      },
      "source": [
        "And we start adding the layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwqdG98_xRtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Dense(8, activation='relu', input_shape=[X_train.shape[1]]))\n",
        "model.add(layers.Dense(16, activation='relu'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ-_FAW_xjAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output layer\n",
        "model.add(layers.Dense(1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzprTmu5xlDf",
        "colab_type": "text"
      },
      "source": [
        "Notice that we only specify the input shape for the first layer, all layers later will know automatically their input shape from the previous one.\n",
        "\n",
        "The activation parameter here specifies the function we want to perform on top of the layer to calculate the output = activation(X * W + bias). Relu is a activation function that is used to break the linearity of the model. There are many other activation functions but Relu is one of the most popular in this kind of networks.\n",
        "\n",
        "The output layer is simply a layer with one neuron and linear activation function since we are predicting only one continuous value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71W2kbQlx6zk",
        "colab_type": "text"
      },
      "source": [
        "## **3.5-Compiling the Model**\n",
        "\n",
        "After building the network we need to specify two important things: 1) the optimizer and 2) the loss function. The optimizer is responsible for navigating the space to choose the best model parameters, while the loss function is used by the optimizer to know how to move in the search space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQFbH3D_ySn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixiOJIyzyU1H",
        "colab_type": "text"
      },
      "source": [
        "Keras supports other optimizers than RMSprop, and you are supposed to do a trial and error process to choose the best one for your problem. But normally RMSprop works fine with its default parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVMFO38WyYhG",
        "colab_type": "text"
      },
      "source": [
        "The loss function used is the Mean Squared Error which is the average squared error a point is from the mean value. Keras supports other loss functions as well that are chosen based on the problem type.\n",
        "\n",
        "The metrics shown here has nothing to do with the model training. It is just a user friendly value that is easier to evaluate than the main loss value. Example: an absolute value loss is easier for us to evaluate and make sense of than the squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d55pwxZdyj7W",
        "colab_type": "text"
      },
      "source": [
        "## **3.6 Model Training**\n",
        "\n",
        "Let the show begin… All is set, we just have to call the fit method to start training…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w--wt37y1lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXKnPTCI-1GO",
        "colab_type": "text"
      },
      "source": [
        "The fit method takes both the features and the labels, the validation split indicates that the model has to keep 20% of the data as a validation set. The epochs indicate the number of iterations on the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19JW2hHk96ps",
        "colab_type": "text"
      },
      "source": [
        "Look how the validation loss decreased from 648 to 20. Impressive!\n",
        "\n",
        "Let us plot the training and validation error convergence according to the epoch number:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utht3eML-V79",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1TtZxfoUxmV4fdVg-pGScnl5qGV6RpToj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chgZOyDjy5Nk",
        "colab_type": "text"
      },
      "source": [
        "We started with an error of 20K per prediction, and went down to around 3K. This is a very acceptable error value for a housing price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv_i08Jo_EcY",
        "colab_type": "text"
      },
      "source": [
        "##**3.7 Evaluation on Test Data**\n",
        "Model evaluation is super easy in Keras. Check the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lZINwEY_Pa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "33b8e733-033e-4ecd-e1bb-0cf7ad2bd3a9"
      },
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 1ms/step - loss: 24.0712 - mae: 3.1230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24.071184158325195, 3.1230244636535645]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5BwREGA_Y6l",
        "colab_type": "text"
      },
      "source": [
        "The output values represent the loss (Mean Squarred Error) and the metrics (Mean Absolute Error)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuLkXmAl_b9j",
        "colab_type": "text"
      },
      "source": [
        "## **3.8 Model Prediction**\n",
        "Using the model for prediction is simpler than you expect. Have a look:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7sWmfbn_qRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we get a sample data (the first 2 inputs from the training data)\n",
        "to_predict = X_train_scaled[:2]\n",
        "# we call the predict method\n",
        "predictions = model.predict(to_predict)\n",
        "# print the predictions\n",
        "print(predictions)\n",
        "# output\n",
        "# array([[13.272537], [39.808475]], dtype=float32)\n",
        "# print the real values\n",
        "print(y_train[:2])\n",
        "# array([15.2, 42.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "#**References**\n",
        "\n",
        " [1] Deep learning with keras Tutorila part 1\n",
        "https://www.marktechpost.com/2019/06/11/deep-learning-with-keras-tutorial-part-1/?fbclid=IwAR0Mbtuas8dNItrxRQvAmZG2OwYXOe6JkjTLmZUNFokVcDyLYHIlENSOze0\n",
        "\n",
        "[2] Regression with Keras (Deep Learning with Keras – Part 3)\n",
        "https://www.marktechpost.com/2019/06/17/regression-with-keras-deep-learning-with-keras-part-3/?fbclid=IwAR0WEGYwi1nlfK_eaH1qK3JD8ITMYc82Q15uT_vC4DnNUOgJboEn8KLr5_Y\n",
        "\n",
        "[3] Data Pre-processing for Deep Learning models (Deep Learning with Keras – Part 2)\n",
        "https://www.marktechpost.com/2019/06/14/data-pre-processing-for-deep-learning-models-deep-learning-with-keras-part-2/?fbclid=IwAR22EF2_QJDOXwc1bFrgiA-ciKzwkbznmcXm8rf8xMJSqkZpKaAsay-_ZuU\n",
        "\n",
        "[4] Deep Learning with Keras – Part 4: Classification\n",
        "https://www.marktechpost.com/2019/06/24/deep-learning-with-keras-part-4-classification/?fbclid=IwAR1NzkaFT4Ys-QsafSGqx7AgXpwiFt183FU57K1lr19VFubux3I7is1y9og\n",
        "\n",
        "[5] Deep Learning with Keras – Part 5: Convolutional Neural Networks\n",
        "https://www.marktechpost.com/2019/07/04/deep-learning-with-keras-part-5-convolutional-neural-networks/?fbclid=IwAR3TveW1lbQ7um9EyL5LG1Mb0SEU5JjDxumHMC5EgCCA23c2A75Dkyv-qio\n",
        "\n",
        "[6] Deep Learning with Keras – Part 6: Textual Data Preprocessing\n",
        "https://www.marktechpost.com/2019/09/13/deep-learning-with-keras-part-6-textual-data-preprocessing/?fbclid=IwAR04cX9X8R6_Lu88p3NfYDnZlY3h1YRQoPT85viLPEZlDuSb75T-zskyC94\n",
        "\n",
        "[7]Deep Learning with Keras – Part 7: Recurrent Neural Networks\n",
        "https://www.marktechpost.com/2019/10/01/deep-learning-with-keras-part-7-recurrent-neural-networks/?fbclid=IwAR2DdCwqkUfDsHHe-n4VVWiZBtqwIdMqHw38z7qc82koBo6yLrW9wyxwNLU\n"
      ]
    }
  ]
}