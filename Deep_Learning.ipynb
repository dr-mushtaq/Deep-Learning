{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3WxDAXWDcR30XMEEaqA/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-mushtaq/Deep-Learning/blob/master/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQlVDtrthPxr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Course 1: Neural Networks and Deep Learning**"
      ],
      "metadata": {
        "id": "zEkAuj0d9ulF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ğŸ“šChapter2: Logistic Regression as a Neural Network**"
      ],
      "metadata": {
        "id": "WZ2_WBe59927"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent on m Examples**"
      ],
      "metadata": {
        "id": "WCj5zRYB-NMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to minimize the MSE loss function for a linear regression model.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): The training data features.\n",
        "        y (np.ndarray): The training data labels.\n",
        "        learning_rate (float): The learning rate.\n",
        "        num_iterations (int): The number of iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The optimal weights.\n",
        "    \"\"\"\n",
        "    weights = np.zeros(X.shape[1])\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        predictions = np.dot(X, weights)\n",
        "        errors = predictions - y\n",
        "        gradients = np.dot(errors.T, X).T\n",
        "\n",
        "        weights -= learning_rate * gradients\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "ecx50xyH-XMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some synthetic data\n",
        "X = np.random.randn(100, 2)\n",
        "y = 2 * X[:, 0] + 1 + np.random.randn(100)"
      ],
      "metadata": {
        "id": "s5lrBC_C-fLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the linear regression model using gradient descent\n",
        "weights = gradient_descent(X, y, 0.01, 1000)\n",
        "\n",
        "print(\"Optimal weights:\", weights)"
      ],
      "metadata": {
        "id": "YXu01qdB-kPD",
        "outputId": "addafdc5-0591-4dc8-86db-1e72a9d470f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal weights: [ 1.62817141 -0.14618265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š**Chapter3: Python and Vectorization**"
      ],
      "metadata": {
        "id": "NDj-b7m75P6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vectorizing Logistic Regression**"
      ],
      "metadata": {
        "id": "SNZPlTbX5Yj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit  # sigmoid function\n",
        "#2- Define Data:\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])  # features\n",
        "y = np.array([0, 1, 1, 0])  # labels\n",
        "#3. Initialize Theta:\n",
        "theta = np.zeros(X.shape[1])  # initialize weights with zeros"
      ],
      "metadata": {
        "id": "n5sfOWKN5g2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Define Functions:\n",
        "def sigmoid(z):\n",
        "    return expit(z)\n",
        "\n",
        "def hypothesis(X, theta):\n",
        "    return sigmoid(np.dot(X, theta))\n",
        "\n",
        "def cost_function(X, y, theta, lambda_reg=0):\n",
        "    m = X.shape[0]\n",
        "    predictions = hypothesis(X, theta)\n",
        "    cost = -(1.0 / m) * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
        "    reg_cost = (lambda_reg / (2 * m)) * np.sum(np.power(theta[1:], 2))\n",
        "    return cost + reg_cost\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, lambda_reg=0):\n",
        "    m = X.shape[0]\n",
        "    predictions = hypothesis(X, theta)\n",
        "    dz = predictions - y\n",
        "    dw = (1.0 / m) * np.dot(X.T, dz)\n",
        "    dw[1:] += (lambda_reg / m) * theta[1:]"
      ],
      "metadata": {
        "id": "uusajFwJ5rhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Training Loop:\n",
        "alpha = 0.1  # learning rate\n",
        "lambda_reg = 0.1  # regularization parameter\n",
        "iterations = 1000"
      ],
      "metadata": {
        "id": "yAFqsAQb5v8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(iterations):\n",
        "    gradient = gradient_descent(X, y, theta, alpha, lambda_reg)\n",
        "    theta -= alpha * gradient\n",
        "\n",
        "print(\"Final Theta:\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1X4LfJy6SRN",
        "outputId": "8c2d3c0e-3635-4843-deed-0c8994ca89fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Theta: [0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Course 2: Improving Deep Neural Networks**"
      ],
      "metadata": {
        "id": "LMjVe0wd9-AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Chapter 1: Practical Aspects of Deep Learning**</p>"
      ],
      "metadata": {
        "id": "dXHhFUMn-IyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularization**"
      ],
      "metadata": {
        "id": "WAdupeHK-PL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization in Logistic Regression**"
      ],
      "metadata": {
        "id": "gCM5BHzn-Z1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X = X[y != 2]  # Use only two classes for binary classification\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without regularization\n",
        "model = LogisticRegression(penalty='none', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy without regularization:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with L2 regularization:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YNW6t5J-ZE6",
        "outputId": "4612ad18-66d8-4594-dd1c-b97422b15b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without regularization: 1.0\n",
            "Accuracy with L2 regularization: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role of Lambda in Regularization**"
      ],
      "metadata": {
        "id": "lu3pwEPz-yKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Grid search to find the best lambda (C in scikit-learn, where C = 1/lambda)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(LogisticRegression(penalty='l2', solver='lbfgs'), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameter (C):\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJFmOdNz-2Jm",
        "outputId": "287fa62b-69bd-41d2-d36f-7b3751f25e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameter (C): {'C': 0.01}\n",
            "Best cross-validation score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 Regularization for Neural Networks**"
      ],
      "metadata": {
        "id": "q41acvDv--Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Create a simple neural network with L2 regularization\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=4, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnVieGud_DLT",
        "outputId": "d89f241e-a396-4889-d333-8b2ebec20537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 2s 42ms/step - loss: 1.6262 - accuracy: 0.4750 - val_loss: 1.3614 - val_accuracy: 0.6000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.3007 - accuracy: 0.6250 - val_loss: 1.2801 - val_accuracy: 0.4000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.1785 - accuracy: 0.5500 - val_loss: 1.1300 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0626 - accuracy: 0.9500 - val_loss: 0.9676 - val_accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.9460 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.8398 - accuracy: 1.0000 - val_loss: 0.7742 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7476 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6661 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6010 - accuracy: 1.0000 - val_loss: 0.5451 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5444 - accuracy: 1.0000 - val_loss: 0.5031 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4608 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4296 - accuracy: 1.0000 - val_loss: 0.3999 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4024 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3797 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3599 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3425 - accuracy: 1.0000 - val_loss: 0.3225 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3273 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.2979 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3007 - accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2895 - accuracy: 1.0000 - val_loss: 0.2756 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2791 - accuracy: 1.0000 - val_loss: 0.2663 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2699 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2614 - accuracy: 1.0000 - val_loss: 0.2494 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2534 - accuracy: 1.0000 - val_loss: 0.2418 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2462 - accuracy: 1.0000 - val_loss: 0.2360 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2393 - accuracy: 1.0000 - val_loss: 0.2293 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2331 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2272 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2218 - accuracy: 1.0000 - val_loss: 0.2128 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2167 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2120 - accuracy: 1.0000 - val_loss: 0.2031 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2076 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2033 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1992 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1955 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1920 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1886 - accuracy: 1.0000 - val_loss: 0.1816 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1854 - accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1824 - accuracy: 1.0000 - val_loss: 0.1756 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1794 - accuracy: 1.0000 - val_loss: 0.1726 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1765 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1739 - accuracy: 1.0000 - val_loss: 0.1679 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1713 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1689 - accuracy: 1.0000 - val_loss: 0.1628 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1665 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1642 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1599 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1581 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1559 - accuracy: 1.0000 - val_loss: 0.1499 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1541 - accuracy: 1.0000 - val_loss: 0.1485 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 1.0000 - val_loss: 0.1464 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1504 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 1.0000 - val_loss: 0.1415 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1455 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1437 - accuracy: 1.0000 - val_loss: 0.1384 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1424 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1410 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1393 - accuracy: 1.0000 - val_loss: 0.1343 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1379 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1368 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1341 - accuracy: 1.0000 - val_loss: 0.1290 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1328 - accuracy: 1.0000 - val_loss: 0.1281 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1305 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1294 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1283 - accuracy: 1.0000 - val_loss: 0.1239 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 0.1222 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1262 - accuracy: 1.0000 - val_loss: 0.1213 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1241 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1224 - accuracy: 1.0000 - val_loss: 0.1182 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1214 - accuracy: 1.0000 - val_loss: 0.1170 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1205 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.1142 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1180 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1172 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1157 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1153 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1128 - accuracy: 1.0000 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1112 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1103 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1092 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1089 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1071 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1066 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1062 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7970b60898a0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dropout Regularization**"
      ],
      "metadata": {
        "id": "Np8igTc5rvtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28, 1)))  # Flatten the input image\n",
        "model.add(Dense(512, activation='relu'))  # Fully connected layer with 512 units\n",
        "model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
        "model.add(Dense(512, activation='relu'))  # Another fully connected layer with 512 units\n",
        "model.add(Dropout(0.5))  # Another dropout layer with 50% dropout rate\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer with 10 units (one for each class)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "id": "yPT9orEmsbua",
        "outputId": "e39aac91-6564-4ed9-e469-f3fdaaf98a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.8107 - loss: 0.5970 - val_accuracy: 0.9605 - val_loss: 0.1309\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - accuracy: 0.9497 - loss: 0.1672 - val_accuracy: 0.9722 - val_loss: 0.0918\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9606 - loss: 0.1260 - val_accuracy: 0.9742 - val_loss: 0.0794\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9678 - loss: 0.1063 - val_accuracy: 0.9775 - val_loss: 0.0734\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.9709 - loss: 0.0934 - val_accuracy: 0.9772 - val_loss: 0.0693\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9726 - loss: 0.0835 - val_accuracy: 0.9796 - val_loss: 0.0660\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9772 - loss: 0.0740 - val_accuracy: 0.9772 - val_loss: 0.0703\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9779 - loss: 0.0724 - val_accuracy: 0.9806 - val_loss: 0.0689\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0636 - val_accuracy: 0.9807 - val_loss: 0.0671\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9804 - loss: 0.0625 - val_accuracy: 0.9818 - val_loss: 0.0612\n",
            "Test loss: 0.061206500977277756 / Test accuracy: 0.9818000197410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Other Regularization Methods**"
      ],
      "metadata": {
        "id": "cyJ6lFrPmDwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with data augmentation and early stopping\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=50,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0utdtXBcmSMe",
        "outputId": "904b4c47-9b67-4fb2-b0c2-b2ae68a591f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 124ms/step - accuracy: 0.2498 - loss: 2.0027 - val_accuracy: 0.4295 - val_loss: 1.6068\n",
            "Epoch 2/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 122ms/step - accuracy: 0.3858 - loss: 1.6825 - val_accuracy: 0.5037 - val_loss: 1.3678\n",
            "Epoch 3/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 120ms/step - accuracy: 0.4308 - loss: 1.5696 - val_accuracy: 0.5493 - val_loss: 1.2456\n",
            "Epoch 4/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 116ms/step - accuracy: 0.4567 - loss: 1.5131 - val_accuracy: 0.5528 - val_loss: 1.2435\n",
            "Epoch 5/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 116ms/step - accuracy: 0.4721 - loss: 1.4637 - val_accuracy: 0.5337 - val_loss: 1.3389\n",
            "Epoch 6/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 115ms/step - accuracy: 0.4836 - loss: 1.4295 - val_accuracy: 0.5787 - val_loss: 1.1794\n",
            "Epoch 7/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 118ms/step - accuracy: 0.4985 - loss: 1.4083 - val_accuracy: 0.6251 - val_loss: 1.0693\n",
            "Epoch 8/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 117ms/step - accuracy: 0.5031 - loss: 1.3865 - val_accuracy: 0.5845 - val_loss: 1.1649\n",
            "Epoch 9/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 115ms/step - accuracy: 0.5149 - loss: 1.3628 - val_accuracy: 0.6240 - val_loss: 1.0711\n",
            "Epoch 10/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 117ms/step - accuracy: 0.5273 - loss: 1.3382 - val_accuracy: 0.6364 - val_loss: 1.0278\n",
            "Epoch 11/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5346 - loss: 1.3219 - val_accuracy: 0.6271 - val_loss: 1.0530\n",
            "Epoch 12/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5426 - loss: 1.2982 - val_accuracy: 0.6488 - val_loss: 1.0072\n",
            "Epoch 13/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 117ms/step - accuracy: 0.5428 - loss: 1.2903 - val_accuracy: 0.6142 - val_loss: 1.1176\n",
            "Epoch 14/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5432 - loss: 1.2886 - val_accuracy: 0.6479 - val_loss: 0.9970\n",
            "Epoch 15/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5550 - loss: 1.2720 - val_accuracy: 0.6508 - val_loss: 0.9854\n",
            "Epoch 16/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 120ms/step - accuracy: 0.5499 - loss: 1.2704 - val_accuracy: 0.6562 - val_loss: 0.9934\n",
            "Epoch 17/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 119ms/step - accuracy: 0.5564 - loss: 1.2481 - val_accuracy: 0.6637 - val_loss: 0.9684\n",
            "Epoch 18/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5558 - loss: 1.2505 - val_accuracy: 0.6454 - val_loss: 1.0075\n",
            "Epoch 19/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 115ms/step - accuracy: 0.5650 - loss: 1.2254 - val_accuracy: 0.6648 - val_loss: 0.9620\n",
            "Epoch 20/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 117ms/step - accuracy: 0.5721 - loss: 1.2178 - val_accuracy: 0.6758 - val_loss: 0.9363\n",
            "Epoch 21/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5694 - loss: 1.2167 - val_accuracy: 0.6600 - val_loss: 0.9623\n",
            "Epoch 22/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5698 - loss: 1.2134 - val_accuracy: 0.6635 - val_loss: 0.9743\n",
            "Epoch 23/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 114ms/step - accuracy: 0.5763 - loss: 1.2020 - val_accuracy: 0.6588 - val_loss: 0.9845\n",
            "Epoch 24/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5813 - loss: 1.1933 - val_accuracy: 0.6730 - val_loss: 0.9296\n",
            "Epoch 25/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5738 - loss: 1.2015 - val_accuracy: 0.6752 - val_loss: 0.9321\n",
            "Epoch 26/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5873 - loss: 1.1808 - val_accuracy: 0.6557 - val_loss: 0.9905\n",
            "Epoch 27/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5866 - loss: 1.1796 - val_accuracy: 0.6692 - val_loss: 0.9535\n",
            "Epoch 28/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 117ms/step - accuracy: 0.5875 - loss: 1.1680 - val_accuracy: 0.6770 - val_loss: 0.9361\n",
            "Epoch 29/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 120ms/step - accuracy: 0.5964 - loss: 1.1514 - val_accuracy: 0.7051 - val_loss: 0.8495\n",
            "Epoch 30/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 119ms/step - accuracy: 0.5901 - loss: 1.1756 - val_accuracy: 0.6800 - val_loss: 0.9033\n",
            "Epoch 31/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 119ms/step - accuracy: 0.5928 - loss: 1.1582 - val_accuracy: 0.6762 - val_loss: 0.9430\n",
            "Epoch 32/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5922 - loss: 1.1530 - val_accuracy: 0.6824 - val_loss: 0.9116\n",
            "Epoch 33/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 119ms/step - accuracy: 0.5930 - loss: 1.1555 - val_accuracy: 0.6937 - val_loss: 0.8832\n",
            "Epoch 34/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 118ms/step - accuracy: 0.5988 - loss: 1.1413 - val_accuracy: 0.6840 - val_loss: 0.9126\n",
            "313/313 - 3s - 10ms/step - accuracy: 0.7051 - loss: 0.8495\n",
            "Test accuracy: 0.7051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Numerical Approximation of Gradients**"
      ],
      "metadata": {
        "id": "xjTFX6QajrC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data: house size (x) and price (y)\n",
        "np.random.seed(0)\n",
        "x = 2 * np.random.rand(100, 1)  # house size (100 houses)\n",
        "y = 4 + 3 * x + np.random.randn(100, 1)  # house price with some noise\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.random.randn(1)  # weight\n",
        "b = np.random.randn(1)  # bias\n",
        "\n",
        "# Define learning rate and epsilon for gradient checking\n",
        "learning_rate = 0.01\n",
        "epsilon = 1e-4\n",
        "\n",
        "# Define the model function y = w * x + b\n",
        "def predict(x, w, b):\n",
        "    return w * x + b\n",
        "\n",
        "# Define the loss function (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Compute gradients for w and b (theoretical gradients)\n",
        "def compute_gradients(x, y, y_pred, w, b):\n",
        "    m = len(x)\n",
        "    dw = (2/m) * np.sum((y_pred - y) * x)\n",
        "    db = (2/m) * np.sum(y_pred - y)\n",
        "    return dw, db\n",
        "\n",
        "# Numerical approximation of gradients using two-sided difference formula\n",
        "def numerical_gradient(x, y, w, b, epsilon, param='w'):\n",
        "    if param == 'w':\n",
        "        w_plus = w + epsilon\n",
        "        w_minus = w - epsilon\n",
        "        loss_plus = compute_loss(y, predict(x, w_plus, b))\n",
        "        loss_minus = compute_loss(y, predict(x, w_minus, b))\n",
        "        return (loss_plus - loss_minus) / (2 * epsilon)\n",
        "    elif param == 'b':\n",
        "        b_plus = b + epsilon\n",
        "        b_minus = b - epsilon\n",
        "        loss_plus = compute_loss(y, predict(x, w, b_plus))\n",
        "        loss_minus = compute_loss(y, predict(x, w, b_minus))\n",
        "        return (loss_plus - loss_minus) / (2 * epsilon)\n",
        "\n",
        "# Perform one step of gradient descent and gradient checking\n",
        "y_pred = predict(x, w, b)\n",
        "loss = compute_loss(y, y_pred)\n",
        "\n",
        "# Theoretical gradients\n",
        "dw_theoretical, db_theoretical = compute_gradients(x, y, y_pred, w, b)\n",
        "\n",
        "# Numerical gradients\n",
        "dw_numerical = numerical_gradient(x, y, w, b, epsilon, param='w')\n",
        "db_numerical = numerical_gradient(x, y, w, b, epsilon, param='b')\n",
        "\n",
        "# Print the results for comparison\n",
        "print(f\"Theoretical gradient for w: {dw_theoretical}\")\n",
        "print(f\"Numerical gradient for w: {dw_numerical}\")\n",
        "print(f\"Theoretical gradient for b: {db_theoretical}\")\n",
        "print(f\"Numerical gradient for b: {db_numerical}\")\n",
        "\n",
        "# Ensure the theoretical and numerical gradients are close\n",
        "assert np.isclose(dw_theoretical, dw_numerical, atol=1e-5), \"Gradients for w don't match!\"\n",
        "assert np.isclose(db_theoretical, db_numerical, atol=1e-5), \"Gradients for b don't match!\"\n"
      ],
      "metadata": {
        "id": "Dw1mmS3sj0vh",
        "outputId": "c82b801e-a095-4627-9397-95e9711809a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical gradient for w: -18.735813085036366\n",
            "Numerical gradient for w: -18.735813085015707\n",
            "Theoretical gradient for b: -17.477561911002848\n",
            "Numerical gradient for b: -17.47756191100791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Checking**"
      ],
      "metadata": {
        "id": "NsZ6etulrNon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple cost function J(Î¸) = Î¸^2\n",
        "def cost_function(theta):\n",
        "    return theta ** 2\n",
        "\n",
        "# Analytical gradient dJ/dÎ¸ = 2Î¸\n",
        "def analytical_gradient(theta):\n",
        "    return 2 * theta\n",
        "\n",
        "# Numerical gradient using two-sided difference approximation\n",
        "def numerical_gradient(theta, epsilon=1e-7):\n",
        "    return (cost_function(theta + epsilon) - cost_function(theta - epsilon)) / (2 * epsilon)\n",
        "\n",
        "# Function to perform gradient checking\n",
        "def gradient_check(theta, epsilon=1e-7):\n",
        "    # Calculate both the analytical and numerical gradients\n",
        "    analytical_grad = analytical_gradient(theta)\n",
        "    numerical_grad = numerical_gradient(theta, epsilon)\n",
        "\n",
        "    # Compare them\n",
        "    diff = np.abs(analytical_grad - numerical_grad)\n",
        "    if diff < 1e-7:\n",
        "        print(f\"Gradient check passed! Difference: {diff}\")\n",
        "    else:\n",
        "        print(f\"Gradient check failed! Difference: {diff}\")\n",
        "\n",
        "# Test the gradient check with a value of Î¸\n",
        "theta_value = 1.5\n",
        "gradient_check(theta_value)"
      ],
      "metadata": {
        "id": "w46FWYsRrbyO",
        "outputId": "d00c7946-f326-4d57-b716-9043c9fbbd8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient check passed! Difference: 1.7516015304863686e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Chapter 2:Optimization Algorithms**</p>"
      ],
      "metadata": {
        "id": "YC-uig4MHzGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SGD with Momentum**"
      ],
      "metadata": {
        "id": "WA3_JIgEK6f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the Neural Network\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)),  # Flatten the 28x28 image\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Configuring SGD Optimizer with Momentum\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the Model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "F70etdLMLH3b",
        "outputId": "ab63b2b1-72dc-42c3-ef2f-2e1ad8b9dd45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8209 - loss: 0.5678\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9528 - loss: 0.1546\n",
            "Epoch 3/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RMSprop**"
      ],
      "metadata": {
        "id": "SJ8SXMOOKbMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the Neural Network\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)),  # Flatten the 28x28 image\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Configuring RMSprop Optimizer\n",
        "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08)\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the Model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "Tvob2eErKmey",
        "outputId": "1a6ea1d4-1bf7-4172-d619-65ce6320882f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8613 - loss: 0.4816\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9566 - loss: 0.1492\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.1073\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9732 - loss: 0.0898\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9769 - loss: 0.0774\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0670\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9832 - loss: 0.0592\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0542\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9862 - loss: 0.0492\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0446\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec250204610>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **-Adam Optimization Algorithm**"
      ],
      "metadata": {
        "id": "0JAwbtQkIFHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Training the Model:**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ta7Mr09bJKwE"
      }
    },
    {
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Building the Neural Network:\n",
        "# Change input shape to match MNIST image shape (28, 28, 1)\n",
        "# Add a Flatten layer to convert the image to a 1D vector\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)), # Flatten the 28x28 image\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Configuring Adam Optimizer:\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "# Compiling the Model:\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# Training the Model:\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbhsloUmJlMD",
        "outputId": "08bd0431-dfa2-46ac-8068-6ca710e9303a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8511 - loss: 0.5126\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1403\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.0995\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9774 - loss: 0.0761\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9798 - loss: 0.0634\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.9835 - loss: 0.0520\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0441\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - accuracy: 0.9882 - loss: 0.0372\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0306\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9912 - loss: 0.0288\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec25034d690>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning Rate Decay**"
      ],
      "metadata": {
        "id": "OHj4sNnIz9aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Objective function: f(x) = (x - 3)^2\n",
        "def loss_function(x):\n",
        "    return (x - 3) ** 2\n",
        "\n",
        "# Gradient of the loss function: f'(x) = 2(x - 3)\n",
        "def gradient(x):\n",
        "    return 2 * (x - 3)\n",
        "\n",
        "# Hyperparameters\n",
        "initial_lr = 0.2          # Initial learning rate (alpha_0)\n",
        "decay_rate = 0.1          # Decay rate\n",
        "epochs = 50               # Number of training iterations\n",
        "x = 0                     # Initial value of x\n",
        "\n",
        "# Store values for visualization\n",
        "x_values = []\n",
        "loss_values = []\n",
        "lr_values = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    lr = initial_lr / (1 + decay_rate * epoch)  # Apply learning rate decay\n",
        "    grad = gradient(x)\n",
        "    x = x - lr * grad  # Gradient descent update\n",
        "\n",
        "    # Track values\n",
        "    x_values.append(x)\n",
        "    loss_values.append(loss_function(x))\n",
        "    lr_values.append(lr)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: x = {x:.4f}, Loss = {loss_function(x):.4f}, LR = {lr:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), loss_values, marker='o')\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(epochs), lr_values, marker='x', color='orange')\n",
        "plt.title(\"Learning Rate Decay\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tALD-Oad0KlL",
        "outputId": "12c2aa73-582d-4b4d-98e1-a03f3034cdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: x = 1.2000, Loss = 3.2400, LR = 0.2000\n",
            "Epoch 02: x = 1.8545, Loss = 1.3121, LR = 0.1818\n",
            "Epoch 03: x = 2.2364, Loss = 0.5831, LR = 0.1667\n",
            "Epoch 04: x = 2.4713, Loss = 0.2795, LR = 0.1538\n",
            "Epoch 05: x = 2.6224, Loss = 0.1426, LR = 0.1429\n",
            "Epoch 06: x = 2.7231, Loss = 0.0767, LR = 0.1333\n",
            "Epoch 07: x = 2.7923, Loss = 0.0431, LR = 0.1250\n",
            "Epoch 08: x = 2.8412, Loss = 0.0252, LR = 0.1176\n",
            "Epoch 09: x = 2.8765, Loss = 0.0153, LR = 0.1111\n",
            "Epoch 10: x = 2.9025, Loss = 0.0095, LR = 0.1053\n",
            "Epoch 11: x = 2.9220, Loss = 0.0061, LR = 0.1000\n",
            "Epoch 12: x = 2.9368, Loss = 0.0040, LR = 0.0952\n",
            "Epoch 13: x = 2.9483, Loss = 0.0027, LR = 0.0909\n",
            "Epoch 14: x = 2.9573, Loss = 0.0018, LR = 0.0870\n",
            "Epoch 15: x = 2.9644, Loss = 0.0013, LR = 0.0833\n",
            "Epoch 16: x = 2.9701, Loss = 0.0009, LR = 0.0800\n",
            "Epoch 17: x = 2.9747, Loss = 0.0006, LR = 0.0769\n",
            "Epoch 18: x = 2.9785, Loss = 0.0005, LR = 0.0741\n",
            "Epoch 19: x = 2.9815, Loss = 0.0003, LR = 0.0714\n",
            "Epoch 20: x = 2.9841, Loss = 0.0003, LR = 0.0690\n",
            "Epoch 21: x = 2.9862, Loss = 0.0002, LR = 0.0667\n",
            "Epoch 22: x = 2.9880, Loss = 0.0001, LR = 0.0645\n",
            "Epoch 23: x = 2.9895, Loss = 0.0001, LR = 0.0625\n",
            "Epoch 24: x = 2.9908, Loss = 0.0001, LR = 0.0606\n",
            "Epoch 25: x = 2.9918, Loss = 0.0001, LR = 0.0588\n",
            "Epoch 26: x = 2.9928, Loss = 0.0001, LR = 0.0571\n",
            "Epoch 27: x = 2.9936, Loss = 0.0000, LR = 0.0556\n",
            "Epoch 28: x = 2.9943, Loss = 0.0000, LR = 0.0541\n",
            "Epoch 29: x = 2.9949, Loss = 0.0000, LR = 0.0526\n",
            "Epoch 30: x = 2.9954, Loss = 0.0000, LR = 0.0513\n",
            "Epoch 31: x = 2.9959, Loss = 0.0000, LR = 0.0500\n",
            "Epoch 32: x = 2.9963, Loss = 0.0000, LR = 0.0488\n",
            "Epoch 33: x = 2.9966, Loss = 0.0000, LR = 0.0476\n",
            "Epoch 34: x = 2.9969, Loss = 0.0000, LR = 0.0465\n",
            "Epoch 35: x = 2.9972, Loss = 0.0000, LR = 0.0455\n",
            "Epoch 36: x = 2.9975, Loss = 0.0000, LR = 0.0444\n",
            "Epoch 37: x = 2.9977, Loss = 0.0000, LR = 0.0435\n",
            "Epoch 38: x = 2.9979, Loss = 0.0000, LR = 0.0426\n",
            "Epoch 39: x = 2.9981, Loss = 0.0000, LR = 0.0417\n",
            "Epoch 40: x = 2.9982, Loss = 0.0000, LR = 0.0408\n",
            "Epoch 41: x = 2.9984, Loss = 0.0000, LR = 0.0400\n",
            "Epoch 42: x = 2.9985, Loss = 0.0000, LR = 0.0392\n",
            "Epoch 43: x = 2.9986, Loss = 0.0000, LR = 0.0385\n",
            "Epoch 44: x = 2.9987, Loss = 0.0000, LR = 0.0377\n",
            "Epoch 45: x = 2.9988, Loss = 0.0000, LR = 0.0370\n",
            "Epoch 46: x = 2.9989, Loss = 0.0000, LR = 0.0364\n",
            "Epoch 47: x = 2.9990, Loss = 0.0000, LR = 0.0357\n",
            "Epoch 48: x = 2.9990, Loss = 0.0000, LR = 0.0351\n",
            "Epoch 49: x = 2.9991, Loss = 0.0000, LR = 0.0345\n",
            "Epoch 50: x = 2.9992, Loss = 0.0000, LR = 0.0339\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Chapter 3:Hyperparameter tunning , Batch Normalization and Programming Frameworks**</p>"
      ],
      "metadata": {
        "id": "4Ej1-5kiksHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Quick Python Example: Log-Scale Sampling for Learning Rate**"
      ],
      "metadata": {
        "id": "3mYbITtRlBKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Sample 5 learning rates on a log scale from 1e-4 to 1\n",
        "def sample_log_uniform(a=-4, b=0, size=5):\n",
        "    r = np.random.uniform(a, b, size)\n",
        "    return 10 ** r\n",
        "\n",
        "learning_rates = sample_log_uniform()\n",
        "print(\"Sampled learning rates:\", learning_rates)\n",
        "\n",
        "# Evaluate each learning rate\n",
        "for lr in learning_rates:\n",
        "    model = LogisticRegression(C=1/lr, max_iter=200)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"Î± = {lr:.1e}, Accuracy = {scores.mean():.3f}\")\n"
      ],
      "metadata": {
        "id": "2yjePuv6lmjL",
        "outputId": "be72224d-7db8-43e2-ce5c-c85e6820b428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled learning rates: [4.81111034e-03 7.25901464e-01 3.36479181e-01 1.55438260e-04\n",
            " 2.43092902e-01]\n",
            "Î± = 4.8e-03, Accuracy = 0.973\n",
            "Î± = 7.3e-01, Accuracy = 0.973\n",
            "Î± = 3.4e-01, Accuracy = 0.973\n",
            "Î± = 1.6e-04, Accuracy = 0.973\n",
            "Î± = 2.4e-01, Accuracy = 0.973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalizing Activations in a Network**"
      ],
      "metadata": {
        "id": "zK8qG12f_tsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s see how normalization affects training using a built-in dataset and a simple neural network with and without batch normalization."
      ],
      "metadata": {
        "id": "kJBflC_v_7BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = to_categorical(digits.target)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize inputs\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build a model with Batch Normalization\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(64,)),\n",
        "    BatchNormalization(),  # Apply batch normalization after the layer\n",
        "    ReLU(),                # Then apply activation\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "q_MZ9TVw_6S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ” What to Observe\n",
        "\n",
        "Youâ€™ll notice that training becomes more stable, often faster, and reaches better accuracy compared to a model without BatchNormalization()."
      ],
      "metadata": {
        "id": "A3zbniNnAOXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**References**</p>"
      ],
      "metadata": {
        "id": "QRdJWIDpkvjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   [Understand Deep Learning](https://udlbook.github.io/udlbook/)\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "H3oT0mCSmEal"
      }
    }
  ]
}