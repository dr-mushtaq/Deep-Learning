{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJLWlBQRt/dm0gValec+4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-mushtaq/Deep-Learning/blob/master/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQlVDtrthPxr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Course 1: Neural Networks and Deep Learning**"
      ],
      "metadata": {
        "id": "zEkAuj0d9ulF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ğŸ“šChapter2: Logistic Regression as a Neural Network**"
      ],
      "metadata": {
        "id": "WZ2_WBe59927"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent on m Examples**"
      ],
      "metadata": {
        "id": "WCj5zRYB-NMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to minimize the MSE loss function for a linear regression model.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): The training data features.\n",
        "        y (np.ndarray): The training data labels.\n",
        "        learning_rate (float): The learning rate.\n",
        "        num_iterations (int): The number of iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The optimal weights.\n",
        "    \"\"\"\n",
        "    weights = np.zeros(X.shape[1])\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        predictions = np.dot(X, weights)\n",
        "        errors = predictions - y\n",
        "        gradients = np.dot(errors.T, X).T\n",
        "\n",
        "        weights -= learning_rate * gradients\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "ecx50xyH-XMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some synthetic data\n",
        "X = np.random.randn(100, 2)\n",
        "y = 2 * X[:, 0] + 1 + np.random.randn(100)"
      ],
      "metadata": {
        "id": "s5lrBC_C-fLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the linear regression model using gradient descent\n",
        "weights = gradient_descent(X, y, 0.01, 1000)\n",
        "\n",
        "print(\"Optimal weights:\", weights)"
      ],
      "metadata": {
        "id": "YXu01qdB-kPD",
        "outputId": "addafdc5-0591-4dc8-86db-1e72a9d470f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal weights: [ 1.62817141 -0.14618265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š**Chapter3: Python and Vectorization**"
      ],
      "metadata": {
        "id": "NDj-b7m75P6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vectorizing Logistic Regression**"
      ],
      "metadata": {
        "id": "SNZPlTbX5Yj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit  # sigmoid function\n",
        "#2- Define Data:\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])  # features\n",
        "y = np.array([0, 1, 1, 0])  # labels\n",
        "#3. Initialize Theta:\n",
        "theta = np.zeros(X.shape[1])  # initialize weights with zeros"
      ],
      "metadata": {
        "id": "n5sfOWKN5g2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Define Functions:\n",
        "def sigmoid(z):\n",
        "    return expit(z)\n",
        "\n",
        "def hypothesis(X, theta):\n",
        "    return sigmoid(np.dot(X, theta))\n",
        "\n",
        "def cost_function(X, y, theta, lambda_reg=0):\n",
        "    m = X.shape[0]\n",
        "    predictions = hypothesis(X, theta)\n",
        "    cost = -(1.0 / m) * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
        "    reg_cost = (lambda_reg / (2 * m)) * np.sum(np.power(theta[1:], 2))\n",
        "    return cost + reg_cost\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, lambda_reg=0):\n",
        "    m = X.shape[0]\n",
        "    predictions = hypothesis(X, theta)\n",
        "    dz = predictions - y\n",
        "    dw = (1.0 / m) * np.dot(X.T, dz)\n",
        "    dw[1:] += (lambda_reg / m) * theta[1:]"
      ],
      "metadata": {
        "id": "uusajFwJ5rhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Training Loop:\n",
        "alpha = 0.1  # learning rate\n",
        "lambda_reg = 0.1  # regularization parameter\n",
        "iterations = 1000"
      ],
      "metadata": {
        "id": "yAFqsAQb5v8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(iterations):\n",
        "    gradient = gradient_descent(X, y, theta, alpha, lambda_reg)\n",
        "    theta -= alpha * gradient\n",
        "\n",
        "print(\"Final Theta:\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1X4LfJy6SRN",
        "outputId": "8c2d3c0e-3635-4843-deed-0c8994ca89fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Theta: [0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Course 2: Improving Deep Neural Networks**"
      ],
      "metadata": {
        "id": "LMjVe0wd9-AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Chapter 1: Practical Aspects of Deep Learning**</p>"
      ],
      "metadata": {
        "id": "dXHhFUMn-IyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularization**"
      ],
      "metadata": {
        "id": "WAdupeHK-PL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization in Logistic Regression**"
      ],
      "metadata": {
        "id": "gCM5BHzn-Z1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X = X[y != 2]  # Use only two classes for binary classification\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without regularization\n",
        "model = LogisticRegression(penalty='none', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy without regularization:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with L2 regularization:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YNW6t5J-ZE6",
        "outputId": "4612ad18-66d8-4594-dd1c-b97422b15b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without regularization: 1.0\n",
            "Accuracy with L2 regularization: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role of Lambda in Regularization**"
      ],
      "metadata": {
        "id": "lu3pwEPz-yKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Grid search to find the best lambda (C in scikit-learn, where C = 1/lambda)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(LogisticRegression(penalty='l2', solver='lbfgs'), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameter (C):\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJFmOdNz-2Jm",
        "outputId": "287fa62b-69bd-41d2-d36f-7b3751f25e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameter (C): {'C': 0.01}\n",
            "Best cross-validation score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 Regularization for Neural Networks**"
      ],
      "metadata": {
        "id": "q41acvDv--Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Create a simple neural network with L2 regularization\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=4, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnVieGud_DLT",
        "outputId": "d89f241e-a396-4889-d333-8b2ebec20537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 2s 42ms/step - loss: 1.6262 - accuracy: 0.4750 - val_loss: 1.3614 - val_accuracy: 0.6000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.3007 - accuracy: 0.6250 - val_loss: 1.2801 - val_accuracy: 0.4000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.1785 - accuracy: 0.5500 - val_loss: 1.1300 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0626 - accuracy: 0.9500 - val_loss: 0.9676 - val_accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.9460 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.8398 - accuracy: 1.0000 - val_loss: 0.7742 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7476 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6661 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6010 - accuracy: 1.0000 - val_loss: 0.5451 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5444 - accuracy: 1.0000 - val_loss: 0.5031 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4608 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4296 - accuracy: 1.0000 - val_loss: 0.3999 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4024 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3797 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3599 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3425 - accuracy: 1.0000 - val_loss: 0.3225 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3273 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.2979 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3007 - accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2895 - accuracy: 1.0000 - val_loss: 0.2756 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2791 - accuracy: 1.0000 - val_loss: 0.2663 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2699 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2614 - accuracy: 1.0000 - val_loss: 0.2494 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2534 - accuracy: 1.0000 - val_loss: 0.2418 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2462 - accuracy: 1.0000 - val_loss: 0.2360 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2393 - accuracy: 1.0000 - val_loss: 0.2293 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2331 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2272 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2218 - accuracy: 1.0000 - val_loss: 0.2128 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2167 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2120 - accuracy: 1.0000 - val_loss: 0.2031 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2076 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2033 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1992 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1955 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1920 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1886 - accuracy: 1.0000 - val_loss: 0.1816 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1854 - accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1824 - accuracy: 1.0000 - val_loss: 0.1756 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1794 - accuracy: 1.0000 - val_loss: 0.1726 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1765 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1739 - accuracy: 1.0000 - val_loss: 0.1679 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1713 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1689 - accuracy: 1.0000 - val_loss: 0.1628 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1665 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1642 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1599 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1581 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1559 - accuracy: 1.0000 - val_loss: 0.1499 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1541 - accuracy: 1.0000 - val_loss: 0.1485 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 1.0000 - val_loss: 0.1464 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1504 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 1.0000 - val_loss: 0.1415 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1455 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1437 - accuracy: 1.0000 - val_loss: 0.1384 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1424 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1410 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1393 - accuracy: 1.0000 - val_loss: 0.1343 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1379 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1368 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1341 - accuracy: 1.0000 - val_loss: 0.1290 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1328 - accuracy: 1.0000 - val_loss: 0.1281 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1305 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1294 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1283 - accuracy: 1.0000 - val_loss: 0.1239 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 0.1222 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1262 - accuracy: 1.0000 - val_loss: 0.1213 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1241 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1224 - accuracy: 1.0000 - val_loss: 0.1182 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1214 - accuracy: 1.0000 - val_loss: 0.1170 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1205 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.1142 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1180 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1172 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1157 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1153 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1128 - accuracy: 1.0000 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1112 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1103 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1092 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1089 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1071 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1066 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1062 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7970b60898a0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dropout Regularization**"
      ],
      "metadata": {
        "id": "Np8igTc5rvtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28, 1)))  # Flatten the input image\n",
        "model.add(Dense(512, activation='relu'))  # Fully connected layer with 512 units\n",
        "model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
        "model.add(Dense(512, activation='relu'))  # Another fully connected layer with 512 units\n",
        "model.add(Dropout(0.5))  # Another dropout layer with 50% dropout rate\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer with 10 units (one for each class)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "id": "yPT9orEmsbua",
        "outputId": "e39aac91-6564-4ed9-e469-f3fdaaf98a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.8107 - loss: 0.5970 - val_accuracy: 0.9605 - val_loss: 0.1309\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - accuracy: 0.9497 - loss: 0.1672 - val_accuracy: 0.9722 - val_loss: 0.0918\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9606 - loss: 0.1260 - val_accuracy: 0.9742 - val_loss: 0.0794\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9678 - loss: 0.1063 - val_accuracy: 0.9775 - val_loss: 0.0734\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.9709 - loss: 0.0934 - val_accuracy: 0.9772 - val_loss: 0.0693\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9726 - loss: 0.0835 - val_accuracy: 0.9796 - val_loss: 0.0660\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9772 - loss: 0.0740 - val_accuracy: 0.9772 - val_loss: 0.0703\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9779 - loss: 0.0724 - val_accuracy: 0.9806 - val_loss: 0.0689\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0636 - val_accuracy: 0.9807 - val_loss: 0.0671\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9804 - loss: 0.0625 - val_accuracy: 0.9818 - val_loss: 0.0612\n",
            "Test loss: 0.061206500977277756 / Test accuracy: 0.9818000197410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Other Regularization Methods**"
      ],
      "metadata": {
        "id": "cyJ6lFrPmDwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with data augmentation and early stopping\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=50,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0utdtXBcmSMe",
        "outputId": "904b4c47-9b67-4fb2-b0c2-b2ae68a591f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 124ms/step - accuracy: 0.2498 - loss: 2.0027 - val_accuracy: 0.4295 - val_loss: 1.6068\n",
            "Epoch 2/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 122ms/step - accuracy: 0.3858 - loss: 1.6825 - val_accuracy: 0.5037 - val_loss: 1.3678\n",
            "Epoch 3/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 120ms/step - accuracy: 0.4308 - loss: 1.5696 - val_accuracy: 0.5493 - val_loss: 1.2456\n",
            "Epoch 4/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 116ms/step - accuracy: 0.4567 - loss: 1.5131 - val_accuracy: 0.5528 - val_loss: 1.2435\n",
            "Epoch 5/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 116ms/step - accuracy: 0.4721 - loss: 1.4637 - val_accuracy: 0.5337 - val_loss: 1.3389\n",
            "Epoch 6/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 115ms/step - accuracy: 0.4836 - loss: 1.4295 - val_accuracy: 0.5787 - val_loss: 1.1794\n",
            "Epoch 7/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 118ms/step - accuracy: 0.4985 - loss: 1.4083 - val_accuracy: 0.6251 - val_loss: 1.0693\n",
            "Epoch 8/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 117ms/step - accuracy: 0.5031 - loss: 1.3865 - val_accuracy: 0.5845 - val_loss: 1.1649\n",
            "Epoch 9/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 115ms/step - accuracy: 0.5149 - loss: 1.3628 - val_accuracy: 0.6240 - val_loss: 1.0711\n",
            "Epoch 10/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 117ms/step - accuracy: 0.5273 - loss: 1.3382 - val_accuracy: 0.6364 - val_loss: 1.0278\n",
            "Epoch 11/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5346 - loss: 1.3219 - val_accuracy: 0.6271 - val_loss: 1.0530\n",
            "Epoch 12/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5426 - loss: 1.2982 - val_accuracy: 0.6488 - val_loss: 1.0072\n",
            "Epoch 13/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 117ms/step - accuracy: 0.5428 - loss: 1.2903 - val_accuracy: 0.6142 - val_loss: 1.1176\n",
            "Epoch 14/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5432 - loss: 1.2886 - val_accuracy: 0.6479 - val_loss: 0.9970\n",
            "Epoch 15/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5550 - loss: 1.2720 - val_accuracy: 0.6508 - val_loss: 0.9854\n",
            "Epoch 16/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 120ms/step - accuracy: 0.5499 - loss: 1.2704 - val_accuracy: 0.6562 - val_loss: 0.9934\n",
            "Epoch 17/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 119ms/step - accuracy: 0.5564 - loss: 1.2481 - val_accuracy: 0.6637 - val_loss: 0.9684\n",
            "Epoch 18/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5558 - loss: 1.2505 - val_accuracy: 0.6454 - val_loss: 1.0075\n",
            "Epoch 19/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 115ms/step - accuracy: 0.5650 - loss: 1.2254 - val_accuracy: 0.6648 - val_loss: 0.9620\n",
            "Epoch 20/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 117ms/step - accuracy: 0.5721 - loss: 1.2178 - val_accuracy: 0.6758 - val_loss: 0.9363\n",
            "Epoch 21/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5694 - loss: 1.2167 - val_accuracy: 0.6600 - val_loss: 0.9623\n",
            "Epoch 22/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5698 - loss: 1.2134 - val_accuracy: 0.6635 - val_loss: 0.9743\n",
            "Epoch 23/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 114ms/step - accuracy: 0.5763 - loss: 1.2020 - val_accuracy: 0.6588 - val_loss: 0.9845\n",
            "Epoch 24/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5813 - loss: 1.1933 - val_accuracy: 0.6730 - val_loss: 0.9296\n",
            "Epoch 25/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.5738 - loss: 1.2015 - val_accuracy: 0.6752 - val_loss: 0.9321\n",
            "Epoch 26/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5873 - loss: 1.1808 - val_accuracy: 0.6557 - val_loss: 0.9905\n",
            "Epoch 27/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 115ms/step - accuracy: 0.5866 - loss: 1.1796 - val_accuracy: 0.6692 - val_loss: 0.9535\n",
            "Epoch 28/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 117ms/step - accuracy: 0.5875 - loss: 1.1680 - val_accuracy: 0.6770 - val_loss: 0.9361\n",
            "Epoch 29/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 120ms/step - accuracy: 0.5964 - loss: 1.1514 - val_accuracy: 0.7051 - val_loss: 0.8495\n",
            "Epoch 30/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 119ms/step - accuracy: 0.5901 - loss: 1.1756 - val_accuracy: 0.6800 - val_loss: 0.9033\n",
            "Epoch 31/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 119ms/step - accuracy: 0.5928 - loss: 1.1582 - val_accuracy: 0.6762 - val_loss: 0.9430\n",
            "Epoch 32/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.5922 - loss: 1.1530 - val_accuracy: 0.6824 - val_loss: 0.9116\n",
            "Epoch 33/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 119ms/step - accuracy: 0.5930 - loss: 1.1555 - val_accuracy: 0.6937 - val_loss: 0.8832\n",
            "Epoch 34/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 118ms/step - accuracy: 0.5988 - loss: 1.1413 - val_accuracy: 0.6840 - val_loss: 0.9126\n",
            "313/313 - 3s - 10ms/step - accuracy: 0.7051 - loss: 0.8495\n",
            "Test accuracy: 0.7051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Numerical Approximation of Gradients**"
      ],
      "metadata": {
        "id": "xjTFX6QajrC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data: house size (x) and price (y)\n",
        "np.random.seed(0)\n",
        "x = 2 * np.random.rand(100, 1)  # house size (100 houses)\n",
        "y = 4 + 3 * x + np.random.randn(100, 1)  # house price with some noise\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.random.randn(1)  # weight\n",
        "b = np.random.randn(1)  # bias\n",
        "\n",
        "# Define learning rate and epsilon for gradient checking\n",
        "learning_rate = 0.01\n",
        "epsilon = 1e-4\n",
        "\n",
        "# Define the model function y = w * x + b\n",
        "def predict(x, w, b):\n",
        "    return w * x + b\n",
        "\n",
        "# Define the loss function (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Compute gradients for w and b (theoretical gradients)\n",
        "def compute_gradients(x, y, y_pred, w, b):\n",
        "    m = len(x)\n",
        "    dw = (2/m) * np.sum((y_pred - y) * x)\n",
        "    db = (2/m) * np.sum(y_pred - y)\n",
        "    return dw, db\n",
        "\n",
        "# Numerical approximation of gradients using two-sided difference formula\n",
        "def numerical_gradient(x, y, w, b, epsilon, param='w'):\n",
        "    if param == 'w':\n",
        "        w_plus = w + epsilon\n",
        "        w_minus = w - epsilon\n",
        "        loss_plus = compute_loss(y, predict(x, w_plus, b))\n",
        "        loss_minus = compute_loss(y, predict(x, w_minus, b))\n",
        "        return (loss_plus - loss_minus) / (2 * epsilon)\n",
        "    elif param == 'b':\n",
        "        b_plus = b + epsilon\n",
        "        b_minus = b - epsilon\n",
        "        loss_plus = compute_loss(y, predict(x, w, b_plus))\n",
        "        loss_minus = compute_loss(y, predict(x, w, b_minus))\n",
        "        return (loss_plus - loss_minus) / (2 * epsilon)\n",
        "\n",
        "# Perform one step of gradient descent and gradient checking\n",
        "y_pred = predict(x, w, b)\n",
        "loss = compute_loss(y, y_pred)\n",
        "\n",
        "# Theoretical gradients\n",
        "dw_theoretical, db_theoretical = compute_gradients(x, y, y_pred, w, b)\n",
        "\n",
        "# Numerical gradients\n",
        "dw_numerical = numerical_gradient(x, y, w, b, epsilon, param='w')\n",
        "db_numerical = numerical_gradient(x, y, w, b, epsilon, param='b')\n",
        "\n",
        "# Print the results for comparison\n",
        "print(f\"Theoretical gradient for w: {dw_theoretical}\")\n",
        "print(f\"Numerical gradient for w: {dw_numerical}\")\n",
        "print(f\"Theoretical gradient for b: {db_theoretical}\")\n",
        "print(f\"Numerical gradient for b: {db_numerical}\")\n",
        "\n",
        "# Ensure the theoretical and numerical gradients are close\n",
        "assert np.isclose(dw_theoretical, dw_numerical, atol=1e-5), \"Gradients for w don't match!\"\n",
        "assert np.isclose(db_theoretical, db_numerical, atol=1e-5), \"Gradients for b don't match!\"\n"
      ],
      "metadata": {
        "id": "Dw1mmS3sj0vh",
        "outputId": "c82b801e-a095-4627-9397-95e9711809a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical gradient for w: -18.735813085036366\n",
            "Numerical gradient for w: -18.735813085015707\n",
            "Theoretical gradient for b: -17.477561911002848\n",
            "Numerical gradient for b: -17.47756191100791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Checking**"
      ],
      "metadata": {
        "id": "NsZ6etulrNon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple cost function J(Î¸) = Î¸^2\n",
        "def cost_function(theta):\n",
        "    return theta ** 2\n",
        "\n",
        "# Analytical gradient dJ/dÎ¸ = 2Î¸\n",
        "def analytical_gradient(theta):\n",
        "    return 2 * theta\n",
        "\n",
        "# Numerical gradient using two-sided difference approximation\n",
        "def numerical_gradient(theta, epsilon=1e-7):\n",
        "    return (cost_function(theta + epsilon) - cost_function(theta - epsilon)) / (2 * epsilon)\n",
        "\n",
        "# Function to perform gradient checking\n",
        "def gradient_check(theta, epsilon=1e-7):\n",
        "    # Calculate both the analytical and numerical gradients\n",
        "    analytical_grad = analytical_gradient(theta)\n",
        "    numerical_grad = numerical_gradient(theta, epsilon)\n",
        "\n",
        "    # Compare them\n",
        "    diff = np.abs(analytical_grad - numerical_grad)\n",
        "    if diff < 1e-7:\n",
        "        print(f\"Gradient check passed! Difference: {diff}\")\n",
        "    else:\n",
        "        print(f\"Gradient check failed! Difference: {diff}\")\n",
        "\n",
        "# Test the gradient check with a value of Î¸\n",
        "theta_value = 1.5\n",
        "gradient_check(theta_value)"
      ],
      "metadata": {
        "id": "w46FWYsRrbyO",
        "outputId": "d00c7946-f326-4d57-b716-9043c9fbbd8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient check passed! Difference: 1.7516015304863686e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Chapter 2:Optimization Algorithms**</p>"
      ],
      "metadata": {
        "id": "YC-uig4MHzGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **-Adam Optimization Algorithm**"
      ],
      "metadata": {
        "id": "0JAwbtQkIFHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Training the Model:**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ta7Mr09bJKwE"
      }
    },
    {
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Building the Neural Network:\n",
        "# Change input shape to match MNIST image shape (28, 28, 1)\n",
        "# Add a Flatten layer to convert the image to a 1D vector\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)), # Flatten the 28x28 image\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Configuring Adam Optimizer:\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "# Compiling the Model:\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# Training the Model:\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NbhsloUmJlMD",
        "outputId": "08bd0431-dfa2-46ac-8068-6ca710e9303a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8511 - loss: 0.5126\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1403\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.0995\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9774 - loss: 0.0761\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9798 - loss: 0.0634\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.9835 - loss: 0.0520\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0441\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - accuracy: 0.9882 - loss: 0.0372\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0306\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9912 - loss: 0.0288\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec25034d690>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}